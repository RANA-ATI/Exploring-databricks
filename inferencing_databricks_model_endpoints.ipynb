{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1><b>Databricks Inferencing Serving Endpoints</b></h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```Generic Setup```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import chromadb\n",
    "from openai import OpenAI\n",
    "from pypdf import PdfReader\n",
    "from langchain_databricks import ChatDatabricks\n",
    "from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction\n",
    "from langchain.text_splitter import (\n",
    "    RecursiveCharacterTextSplitter,\n",
    "    SentenceTransformersTokenTextSplitter,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Envs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the values using os.environ\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "DATABRICKS_ENDPOINT = os.getenv(\"DATABRICKS_ENDPOINT\")\n",
    "DB_NAME = os.getenv(\"DB_NAME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **OpenAI Initialization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "  api_key=OPENAI_API_KEY,\n",
    "  base_url=DATABRICKS_ENDPOINT\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **DB Initialization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def db_init_embedd():\n",
    "    embedding_function = SentenceTransformerEmbeddingFunction()\n",
    "\n",
    "    chroma_client = chromadb.Client()\n",
    "\n",
    "    # Instead of just storing it to memory we are now saving it locally.\n",
    "    # chroma_client = chromadb.PersistentClient(path=DB_LOCATION)\n",
    "\n",
    "    # get_or_create_collection : This will either get the collection or creates it\n",
    "    chroma_collection = chroma_client.get_or_create_collection(\n",
    "        DB_NAME, embedding_function=embedding_function\n",
    "    )\n",
    "\n",
    "    return chroma_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_collection = db_init_embedd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = [\"database/demo.pdf\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Chunking Text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Replace non-breaking spaces with regular spaces\n",
    "    text = text.replace('\\xa0', ' ')\n",
    "    # Remove multiple spaces, tabs, or newlines\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Strip leading and trailing spaces\n",
    "    return text.strip()\n",
    "\n",
    "def embeddings_creation(file_paths):\n",
    "    pdf_texts = []\n",
    "    for file_path in file_paths:\n",
    "        reader = PdfReader(file_path)\n",
    "        pdf_texts.extend([clean_text(p.extract_text()) for p in reader.pages if p.extract_text()])\n",
    "\n",
    "    # Filter the empty strings\n",
    "    pdf_texts = [text for text in pdf_texts if text]\n",
    "\n",
    "    character_splitter = RecursiveCharacterTextSplitter(\n",
    "        # It will split on the basis of these below characters like newline etc\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
    "        # If after splitting at separators, it got a big length then it will break down into chunk size of 1000 characters maximum\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=0,\n",
    "    )\n",
    "    \n",
    "    character_split_texts = character_splitter.split_text(\"\\n\\n\".join(pdf_texts))\n",
    "\n",
    "    token_splitter = SentenceTransformersTokenTextSplitter(\n",
    "        chunk_overlap=0, tokens_per_chunk=256\n",
    "    )  # tokens_per_chunk is context window which means that it one chunk would have 256 tokens\n",
    "\n",
    "    # We shall use all the chunks made by character text splitter and we are resplitting them using the token text splitter\n",
    "    token_split_texts = []\n",
    "    for text in character_split_texts:\n",
    "        token_split_texts += token_splitter.split_text(text)\n",
    "\n",
    "    ids = [str(i) for i in range(len(token_split_texts))]\n",
    "\n",
    "    return ids , token_split_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids, token_split_texts = embeddings_creation(file_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Inserting Chunking data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def storing_embeddings_db(chroma_collection, ids, token_split_texts):\n",
    "    chroma_collection.add(ids=ids, documents=token_split_texts)\n",
    "\n",
    "    return \"Stored Embeddings in Vector DB\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_data_to_db = storing_embeddings_db(chroma_collection, ids, token_split_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```Inferencing Langchain vs OpenAI```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **OpenAI**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------- Llama3.1 Databricks --------------------------------\n",
    "def rag(client, chroma_collection, query):\n",
    "    # Here chroma automatically embeds using the embedding function we have used above the query and give retrieved documents\n",
    "    results = chroma_collection.query(query_texts=[query], n_results=5)\n",
    "    retrieved_documents = results[\"documents\"][0]\n",
    "\n",
    "    information = \"\\n\\n\".join(retrieved_documents)\n",
    "\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant. Your users are asking questions about information contained in reports or files. You will be shown the user's question, and the relevant information from the files or reports. Answer the user's question using only this information.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Query: {query} , Information: {information}\"\n",
    "        }\n",
    "        ],\n",
    "        model=\"llama3-1\",\n",
    "        max_tokens=512\n",
    "    )\n",
    "\n",
    "    return chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"what are some countries that are listed in this document?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:chromadb.segment.impl.vector.local_hnsw:Number of requested results 5 is greater than number of elements in index 3, updating n_results = 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the document, the countries listed as neighbors of Great Britain are:\n",
      "\n",
      "1. Denmark (to the north)\n",
      "2. Germany (to the east)\n",
      "3. Switzerland (to the south)\n",
      "4. Austria (to the south)\n",
      "5. France (to the west)\n"
     ]
    }
   ],
   "source": [
    "result = rag(client, chroma_collection, query)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Langchain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:chromadb.segment.impl.vector.local_hnsw:Number of requested results 5 is greater than number of elements in index 3, updating n_results = 3\n"
     ]
    }
   ],
   "source": [
    "query = \"what are some countries that are listed in this document?\"\n",
    "\n",
    "# Here chroma automatically embeds using the embedding function we have used above the query and give retrieved documents\n",
    "results = chroma_collection.query(query_texts=[query], n_results=5)\n",
    "retrieved_documents = results[\"documents\"][0]\n",
    "\n",
    "information = \"\\n\\n\".join(retrieved_documents)\n",
    "\n",
    "template = f\"\"\"\n",
    "            \"prompt\":f\"You are a helpful expert research assistant. Your users are asking questions about information contained in reports or files.\"\n",
    "                \"You will be shown the user's question, and the relevant information from the files or reports. Answer the user's question using only this information.\" \n",
    "                \"Question: {query}. \\n Information: {information}\"\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_model = ChatDatabricks(endpoint=\"llama3-1\", \n",
    "                            temperature=0.5,\n",
    "                            max_tokens=512)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_model_output = chat_model.invoke(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The countries listed in this document are:\n",
      "\n",
      "1. Denmark\n",
      "2. Germany\n",
      "3. Switzerland\n",
      "4. Austria\n",
      "5. France\n"
     ]
    }
   ],
   "source": [
    "# Accessing the content attribute of the AIMessage object\n",
    "content = chat_model_output.content\n",
    "\n",
    "# Print or process the content\n",
    "print(content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
